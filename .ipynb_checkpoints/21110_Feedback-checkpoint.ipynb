{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9e64f138",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in d:\\anaconda\\lib\\site-packages (22.1)\n",
      "Collecting pip\n",
      "  Using cached pip-22.1.2-py3-none-any.whl (2.1 MB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: To modify pip, please run the following command:\n",
      "D:\\Anaconda\\python.exe -m pip install --upgrade pip\n",
      "WARNING: There was an error checking the latest version of pip.\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c02ac881",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow in d:\\anaconda\\lib\\site-packages (2.8.0)\n",
      "Requirement already satisfied: numpy>=1.20 in d:\\anaconda\\lib\\site-packages (from tensorflow) (1.21.5)\n",
      "Requirement already satisfied: six>=1.12.0 in d:\\anaconda\\lib\\site-packages (from tensorflow) (1.16.0)\n",
      "Requirement already satisfied: libclang>=9.0.1 in d:\\anaconda\\lib\\site-packages (from tensorflow) (14.0.1)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in d:\\anaconda\\lib\\site-packages (from tensorflow) (1.42.0)\n",
      "Requirement already satisfied: keras-preprocessing>=1.1.1 in d:\\anaconda\\lib\\site-packages (from tensorflow) (1.1.2)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in d:\\anaconda\\lib\\site-packages (from tensorflow) (1.12.1)\n",
      "Requirement already satisfied: absl-py>=0.4.0 in d:\\anaconda\\lib\\site-packages (from tensorflow) (1.0.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in d:\\anaconda\\lib\\site-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: setuptools in d:\\anaconda\\lib\\site-packages (from tensorflow) (61.2.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in d:\\anaconda\\lib\\site-packages (from tensorflow) (1.1.0)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in d:\\anaconda\\lib\\site-packages (from tensorflow) (3.3.0)\n",
      "Requirement already satisfied: h5py>=2.9.0 in d:\\anaconda\\lib\\site-packages (from tensorflow) (3.6.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in d:\\anaconda\\lib\\site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: tensorboard<2.9,>=2.8 in d:\\anaconda\\lib\\site-packages (from tensorflow) (2.8.0)\n",
      "Requirement already satisfied: tf-estimator-nightly==2.8.0.dev2021122109 in d:\\anaconda\\lib\\site-packages (from tensorflow) (2.8.0.dev2021122109)\n",
      "Requirement already satisfied: flatbuffers>=1.12 in d:\\anaconda\\lib\\site-packages (from tensorflow) (2.0)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in d:\\anaconda\\lib\\site-packages (from tensorflow) (0.25.0)\n",
      "Requirement already satisfied: protobuf>=3.9.2 in d:\\anaconda\\lib\\site-packages (from tensorflow) (3.19.1)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in d:\\anaconda\\lib\\site-packages (from tensorflow) (4.1.1)\n",
      "Requirement already satisfied: gast>=0.2.1 in d:\\anaconda\\lib\\site-packages (from tensorflow) (0.5.3)\n",
      "Requirement already satisfied: keras<2.9,>=2.8.0rc0 in d:\\anaconda\\lib\\site-packages (from tensorflow) (2.8.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in d:\\anaconda\\lib\\site-packages (from astunparse>=1.6.0->tensorflow) (0.37.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in d:\\anaconda\\lib\\site-packages (from tensorboard<2.9,>=2.8->tensorflow) (3.3.4)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in d:\\anaconda\\lib\\site-packages (from tensorboard<2.9,>=2.8->tensorflow) (0.4.6)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in d:\\anaconda\\lib\\site-packages (from tensorboard<2.9,>=2.8->tensorflow) (1.33.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in d:\\anaconda\\lib\\site-packages (from tensorboard<2.9,>=2.8->tensorflow) (0.6.1)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in d:\\anaconda\\lib\\site-packages (from tensorboard<2.9,>=2.8->tensorflow) (1.8.1)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in d:\\anaconda\\lib\\site-packages (from tensorboard<2.9,>=2.8->tensorflow) (2.27.1)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in d:\\anaconda\\lib\\site-packages (from tensorboard<2.9,>=2.8->tensorflow) (2.0.3)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in d:\\anaconda\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow) (4.7.2)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in d:\\anaconda\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow) (4.2.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in d:\\anaconda\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow) (0.2.8)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in d:\\anaconda\\lib\\site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow) (1.3.1)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in d:\\anaconda\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\anaconda\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow) (2021.10.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in d:\\anaconda\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow) (1.26.9)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\anaconda\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow) (3.3)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in d:\\anaconda\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow) (0.4.8)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: There was an error checking the latest version of pip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: oauthlib>=3.0.0 in d:\\anaconda\\lib\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow) (3.2.0)\n",
      "Requirement already satisfied: opencv-python in d:\\anaconda\\lib\\site-packages (4.5.5.64)\n",
      "Requirement already satisfied: numpy>=1.17.3 in d:\\anaconda\\lib\\site-packages (from opencv-python) (1.21.5)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: There was an error checking the latest version of pip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pillow in d:\\anaconda\\lib\\site-packages (9.0.1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: There was an error checking the latest version of pip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-image in d:\\anaconda\\lib\\site-packages (0.19.2)\n",
      "Requirement already satisfied: tifffile>=2019.7.26 in d:\\anaconda\\lib\\site-packages (from scikit-image) (2021.7.2)\n",
      "Requirement already satisfied: numpy>=1.17.0 in d:\\anaconda\\lib\\site-packages (from scikit-image) (1.21.5)\n",
      "Requirement already satisfied: pillow!=7.1.0,!=7.1.1,!=8.3.0,>=6.1.0 in d:\\anaconda\\lib\\site-packages (from scikit-image) (9.0.1)\n",
      "Requirement already satisfied: scipy>=1.4.1 in d:\\anaconda\\lib\\site-packages (from scikit-image) (1.8.0)\n",
      "Requirement already satisfied: networkx>=2.2 in d:\\anaconda\\lib\\site-packages (from scikit-image) (2.8)\n",
      "Requirement already satisfied: PyWavelets>=1.1.1 in d:\\anaconda\\lib\\site-packages (from scikit-image) (1.3.0)\n",
      "Requirement already satisfied: imageio>=2.4.1 in d:\\anaconda\\lib\\site-packages (from scikit-image) (2.9.0)\n",
      "Requirement already satisfied: packaging>=20.0 in d:\\anaconda\\lib\\site-packages (from scikit-image) (21.3)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in d:\\anaconda\\lib\\site-packages (from packaging>=20.0->scikit-image) (3.0.4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: There was an error checking the latest version of pip.\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow\n",
    "!pip install opencv-python    \n",
    "!pip install pillow\n",
    "!pip install scikit-image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "898f82ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1493, 2780, 1583]\n",
      "3747\n",
      "14988\n",
      "Epoch 1/100\n",
      "100/100 [==============================] - 126s 1s/step - loss: 1.0394 - accuracy: 0.4949 - val_loss: 0.7074 - val_accuracy: 0.6841\n",
      "Epoch 2/100\n",
      "100/100 [==============================] - 119s 1s/step - loss: 0.9541 - accuracy: 0.5370 - val_loss: 0.6437 - val_accuracy: 0.7108\n",
      "Epoch 3/100\n",
      "100/100 [==============================] - 119s 1s/step - loss: 0.8794 - accuracy: 0.5850 - val_loss: 0.5700 - val_accuracy: 0.7620\n",
      "Epoch 4/100\n",
      "100/100 [==============================] - 121s 1s/step - loss: 0.6388 - accuracy: 0.7341 - val_loss: 0.5650 - val_accuracy: 0.7663\n",
      "Epoch 5/100\n",
      "100/100 [==============================] - 111s 1s/step - loss: 0.5989 - accuracy: 0.7485 - val_loss: 0.5728 - val_accuracy: 0.7684\n",
      "Epoch 6/100\n",
      "100/100 [==============================] - 106s 1s/step - loss: 0.5744 - accuracy: 0.7591 - val_loss: 0.5677 - val_accuracy: 0.7620\n",
      "Epoch 7/100\n",
      "100/100 [==============================] - 106s 1s/step - loss: 0.5535 - accuracy: 0.7686 - val_loss: 0.5487 - val_accuracy: 0.7727\n",
      "Epoch 8/100\n",
      "100/100 [==============================] - 106s 1s/step - loss: 0.5344 - accuracy: 0.7764 - val_loss: 0.5340 - val_accuracy: 0.7769\n",
      "Epoch 9/100\n",
      "100/100 [==============================] - 106s 1s/step - loss: 0.5217 - accuracy: 0.7826 - val_loss: 0.5390 - val_accuracy: 0.7823\n",
      "Epoch 10/100\n",
      "100/100 [==============================] - 106s 1s/step - loss: 0.5068 - accuracy: 0.7898 - val_loss: 0.5410 - val_accuracy: 0.7705\n",
      "Epoch 11/100\n",
      "100/100 [==============================] - 105s 1s/step - loss: 0.5007 - accuracy: 0.7891 - val_loss: 0.5812 - val_accuracy: 0.7620\n",
      "Epoch 12/100\n",
      "100/100 [==============================] - 105s 1s/step - loss: 0.4914 - accuracy: 0.7940 - val_loss: 0.5861 - val_accuracy: 0.7673\n",
      "Epoch 13/100\n",
      "100/100 [==============================] - 109s 1s/step - loss: 0.4742 - accuracy: 0.8035 - val_loss: 0.5436 - val_accuracy: 0.7844\n",
      "Epoch 14/100\n",
      "100/100 [==============================] - 105s 1s/step - loss: 0.4702 - accuracy: 0.8009 - val_loss: 0.5447 - val_accuracy: 0.7812\n",
      "Epoch 15/100\n",
      "100/100 [==============================] - 106s 1s/step - loss: 0.4538 - accuracy: 0.8104 - val_loss: 0.5693 - val_accuracy: 0.7609\n",
      "Epoch 16/100\n",
      "100/100 [==============================] - 106s 1s/step - loss: 0.4537 - accuracy: 0.8111 - val_loss: 0.5219 - val_accuracy: 0.7844\n",
      "Epoch 17/100\n",
      "100/100 [==============================] - 106s 1s/step - loss: 0.4418 - accuracy: 0.8163 - val_loss: 0.5467 - val_accuracy: 0.7823\n",
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 100, 100, 1)]     0         \n",
      "                                                                 \n",
      " conv2d (Conv2D)             (None, 98, 98, 32)        320       \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 49, 49, 32)       0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 47, 47, 64)        18496     \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 23, 23, 64)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 21, 21, 96)        55392     \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPooling  (None, 10, 10, 96)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 9600)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 50)                480050    \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 50)                2550      \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 3)                 153       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 556,961\n",
      "Trainable params: 556,961\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Test loss: 0.4939601719379425\n",
      "Test accuracy: 0.796928346157074\n"
     ]
    }
   ],
   "source": [
    "def process(X,file):\n",
    "    img=cv2.imread(file,cv2.IMREAD_GRAYSCALE)\n",
    "    X.append(img)\n",
    "    #write your code\n",
    "\n",
    "def normalize(dataset):\n",
    "    dataset=asarray(dataset).astype('float32')\n",
    "    dataset=dataset/255.0\n",
    "    return dataset\n",
    "\n",
    "def augmentation(row,label,Augmented,Y_Augmeneted):\n",
    "    scale_out = skimage.transform.rescale(row, scale=2.0, mode='constant')\n",
    "    scale_out=skimage.transform.resize(scale_out,(100,100))\n",
    "    scale_in= skimage.transform.rescale(row, scale=0.5, mode='constant')\n",
    "    scale_in=skimage.transform.resize(scale_in,(100,100))\n",
    "    rot = skimage.transform.rotate(row, angle=20, mode='reflect')\n",
    "    rot=skimage.transform.resize(rot,(100,100))\n",
    "    images=[row,scale_out,scale_in,rot]\n",
    "    for image in images:\n",
    "        Augmented.append(image)\n",
    "        Y_Augmented.append(label)\n",
    "\n",
    "\n",
    "\n",
    "import skimage\n",
    "from skimage import transform\n",
    "from skimage.transform import resize\n",
    "from skimage.transform import rescale\n",
    "import cv2\n",
    "import glob\n",
    "import random as ra\n",
    "import PIL\n",
    "from PIL import Image\n",
    "from numpy import asarray\n",
    "\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow import keras\n",
    "from keras import layers\n",
    "from keras import utils\n",
    "import sklearn.metrics as metrics\n",
    "import numpy as np\n",
    "from tensorflow.keras import regularizers\n",
    "import math\n",
    "\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "Normal=[]\n",
    "Bacterial=[]\n",
    "Virus=[]\n",
    "classes=[Virus,Bacterial,Normal]\n",
    "Y_Normal=[]\n",
    "Y_Bacterial=[]\n",
    "Y_Virus=[]\n",
    "Y=[Y_Normal,Y_Bacterial,Y_Virus]\n",
    "files=glob.glob('chest_xray_lower_dim2'+'/*.jpeg')\n",
    "ra.shuffle(files)\n",
    "for file in files:\n",
    "    k=-1\n",
    "    if \"virus\" in file:\n",
    "        k=0\n",
    "    elif \"bacteria\" in file:\n",
    "        k=1\n",
    "    else:\n",
    "        k=2\n",
    "    Y[k].append(str(k))\n",
    "    process(classes[k],file)\n",
    "    \n",
    "len_classes=[]\n",
    "for class_ in classes:\n",
    "    len_classes.append(len(class_))\n",
    "print(len_classes)\n",
    "same_length=min(len_classes)\n",
    "evenly=False\n",
    "\n",
    "if evenly:\n",
    "    for i in range(3):\n",
    "        len_classes[i]=same_length\n",
    "\n",
    "for i in range(3):\n",
    "    classes[i]=classes[i][0:len_classes[i]]\n",
    "for i in range(3):\n",
    "    Y[i]=Y[i][0:len_classes[i]]\n",
    "\n",
    "Train=[]\n",
    "Valid=[]\n",
    "Test=[]\n",
    "TVT=[Train,Valid,Test]\n",
    "Y_Train=[]\n",
    "Y_Valid=[]\n",
    "Y_Test=[]\n",
    "Y_TVT=[Y_Train,Y_Valid,Y_Test]\n",
    "\n",
    "bounds=[0,0.64,0.8,1]\n",
    "for i in range(3):\n",
    "    for j in range(3):\n",
    "        TVT[i]+=classes[j][int(bounds[i]*len_classes[j]):int(bounds[i+1]*len_classes[j])]\n",
    "        Y_TVT[i]+=Y[j][int(bounds[i]*len_classes[j]):int(bounds[i+1]*len_classes[j])]\n",
    "        \n",
    "Augmented=[]\n",
    "Y_Augmented=[]\n",
    "\n",
    "for i in range(len(Train)):\n",
    "    augmentation(Train[i],Y_Train[i],Augmented,Y_Augmented)\n",
    "\n",
    "print(len(Train))\n",
    "\n",
    "TVT[0]=Augmented\n",
    "Y_TVT[0]=Y_Augmented\n",
    "\n",
    "for i in range(3):\n",
    "    TVT[i]=normalize(TVT[i])\n",
    "    TVT[i]=TVT[i].reshape(-1,100,100,1)\n",
    "    Y_TVT[i]=keras.utils.to_categorical(Y_TVT[i],3)\n",
    "\n",
    "print(len(Augmented))\n",
    "\n",
    "# epoch=32 steps_per_epoch=32\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "def train(epochs_num,batch_len,conv_neurons,art_neurons):\n",
    "    \n",
    "    def mlp_model():\n",
    "        Inp=layers.Input(shape=(100,100,1))\n",
    "        hidden=Inp\n",
    "        for conv_neuron in conv_neurons:\n",
    "            hidden=layers.Conv2D(conv_neuron,kernel_size=(3,3),padding='valid',activation='relu')(hidden)\n",
    "            hidden=layers.MaxPool2D(pool_size=(2,2),strides=(2,2))(hidden)\n",
    "        hidden=layers.Flatten()(hidden)\n",
    "        inputs=100*100\n",
    "        w=math.sqrt(inputs)\n",
    "        for art_neuron in art_neurons:\n",
    "            hidden=layers.Dense(art_neuron,activation='relu',kernel_regularizer=regularizers.L2(0.0001))(hidden)\n",
    "            #hidden=layers.Dropout(0.2)(hidden)\n",
    "            inputs=30\n",
    "            w=math.sqrt(inputs)\n",
    "        output=layers.Dense(3,activation='softmax',kernel_regularizer=regularizers.L2(0.0001))(hidden)\n",
    "        model=keras.Model(Inp,output)\n",
    "        model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "        return model\n",
    "    \"\"\"\n",
    "    checkpoint = tf.keras.callbacks.ModelCheckpoint(\"weights.hdf5\",monitor='val_accuracy', verbose=1, \n",
    "    save_best_only=True, mode='max')\n",
    "    callbacks_list = [checkpoint]\n",
    "    estimator = KerasClassifier(build_fn=mlp_model, epochs=int(len(X_train)/dividor_epoch), batch_size=batch_len, verbose=2)\n",
    "    kfold = KFold(n_splits=8, shuffle=True, random_state=7)\n",
    "    results = cross_val_score(estimator, X_train, Y_train, cv=kfold)\n",
    "    print(\"MLP: %.2f%% (%.2f%%)\" % (results.mean()*100, results.std()*100))\n",
    "    \"\"\"\n",
    "    callback = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=4,mode='max')\n",
    "    model=mlp_model()\n",
    "    model.fit(TVT[0],Y_TVT[0],validation_data=(TVT[1],Y_TVT[1]),epochs=epochs_num,batch_size=batch_len,callbacks=[callback],verbose=1)\n",
    "    #model.load_weights(\"weights.hdf5\")\n",
    "    model.summary()\n",
    "    score= model.evaluate(TVT[2], Y_TVT[2],verbose=0)\n",
    "    print(\"Test loss:\", score[0])\n",
    "    print(\"Test accuracy:\",score[1])\n",
    "    \n",
    "train(100,150,[32,64,96],[50,50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d1fd56fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1493, 2780, 1583]\n",
      "3747\n",
      "14988\n",
      "Epoch 1/100\n",
      "100/100 [==============================] - 745s 7s/step - loss: 1.0712 - accuracy: 0.4722 - val_loss: 0.9943 - val_accuracy: 0.4749\n",
      "Epoch 2/100\n",
      "100/100 [==============================] - 741s 7s/step - loss: 0.9984 - accuracy: 0.5189 - val_loss: 0.6196 - val_accuracy: 0.7225\n",
      "Epoch 3/100\n",
      "100/100 [==============================] - 771s 8s/step - loss: 0.9554 - accuracy: 0.5417 - val_loss: 0.5324 - val_accuracy: 0.7705\n",
      "Epoch 4/100\n",
      "100/100 [==============================] - 773s 8s/step - loss: 0.9418 - accuracy: 0.5483 - val_loss: 0.4875 - val_accuracy: 0.7919\n",
      "Epoch 5/100\n",
      "100/100 [==============================] - 787s 8s/step - loss: 0.9313 - accuracy: 0.5500 - val_loss: 0.4759 - val_accuracy: 0.7940\n",
      "Epoch 6/100\n",
      "100/100 [==============================] - 792s 8s/step - loss: 0.9305 - accuracy: 0.5520 - val_loss: 0.4526 - val_accuracy: 0.8036\n",
      "Epoch 7/100\n",
      "100/100 [==============================] - 812s 8s/step - loss: 0.9229 - accuracy: 0.5532 - val_loss: 0.4674 - val_accuracy: 0.7972\n",
      "Epoch 8/100\n",
      "100/100 [==============================] - 832s 8s/step - loss: 0.9149 - accuracy: 0.5566 - val_loss: 0.4849 - val_accuracy: 0.7866\n",
      "Epoch 9/100\n",
      "100/100 [==============================] - 806s 8s/step - loss: 0.9155 - accuracy: 0.5564 - val_loss: 0.4698 - val_accuracy: 0.7940\n",
      "Epoch 10/100\n",
      "100/100 [==============================] - 792s 8s/step - loss: 0.9109 - accuracy: 0.5578 - val_loss: 0.4641 - val_accuracy: 0.8026\n",
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 200, 200, 1)]     0         \n",
      "                                                                 \n",
      " conv2d (Conv2D)             (None, 198, 198, 32)      320       \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 99, 99, 32)       0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 97, 97, 64)        18496     \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 48, 48, 64)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 46, 46, 96)        55392     \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPooling  (None, 23, 23, 96)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 21, 21, 128)       110720    \n",
      "                                                                 \n",
      " max_pooling2d_3 (MaxPooling  (None, 10, 10, 128)      0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 12800)             0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 50)                640050    \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 50)                0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 50)                2550      \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 50)                0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 3)                 153       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 827,681\n",
      "Trainable params: 827,681\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Test loss: 0.4922078251838684\n",
      "Test accuracy: 0.7952218651771545\n"
     ]
    }
   ],
   "source": [
    "#200,200\n",
    "def process(X,file):\n",
    "    img=cv2.imread(file,cv2.IMREAD_GRAYSCALE)\n",
    "    X.append(img)\n",
    "    #write your code\n",
    "\n",
    "def normalize(dataset):\n",
    "    dataset=asarray(dataset).astype('float32')\n",
    "    dataset=dataset/255.0\n",
    "    return dataset\n",
    "\n",
    "def augmentation(row,label,Augmented,Y_Augmeneted):\n",
    "    scale_out = skimage.transform.rescale(row, scale=2.0, mode='constant')\n",
    "    scale_out=skimage.transform.resize(scale_out,(200,200))\n",
    "    scale_in= skimage.transform.rescale(row, scale=0.5, mode='constant')\n",
    "    scale_in=skimage.transform.resize(scale_in,(200,200))\n",
    "    rot = skimage.transform.rotate(row, angle=20, mode='reflect')\n",
    "    rot=skimage.transform.resize(rot,(200,200))\n",
    "    images=[row,scale_out,scale_in,rot]\n",
    "    for image in images:\n",
    "        Augmented.append(image)\n",
    "        Y_Augmented.append(label)\n",
    "\n",
    "\n",
    "\n",
    "import skimage\n",
    "from skimage import transform\n",
    "from skimage.transform import resize\n",
    "from skimage.transform import rescale\n",
    "import cv2\n",
    "import glob\n",
    "import random as ra\n",
    "import PIL\n",
    "from PIL import Image\n",
    "from numpy import asarray\n",
    "\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow import keras\n",
    "from keras import layers\n",
    "from keras import utils\n",
    "import sklearn.metrics as metrics\n",
    "import numpy as np\n",
    "from tensorflow.keras import regularizers\n",
    "import math\n",
    "\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "Normal=[]\n",
    "Bacterial=[]\n",
    "Virus=[]\n",
    "classes=[Virus,Bacterial,Normal]\n",
    "Y_Normal=[]\n",
    "Y_Bacterial=[]\n",
    "Y_Virus=[]\n",
    "Y=[Y_Normal,Y_Bacterial,Y_Virus]\n",
    "files=glob.glob('chest_xray_lower_dim200x200'+'/*.jpeg')\n",
    "ra.shuffle(files)\n",
    "for file in files:\n",
    "    k=-1\n",
    "    if \"virus\" in file:\n",
    "        k=0\n",
    "    elif \"bacteria\" in file:\n",
    "        k=1\n",
    "    else:\n",
    "        k=2\n",
    "    Y[k].append(str(k))\n",
    "    process(classes[k],file)\n",
    "    \n",
    "len_classes=[]\n",
    "for class_ in classes:\n",
    "    len_classes.append(len(class_))\n",
    "print(len_classes)\n",
    "same_length=min(len_classes)\n",
    "evenly=False\n",
    "\n",
    "if evenly:\n",
    "    for i in range(3):\n",
    "        len_classes[i]=same_length\n",
    "\n",
    "for i in range(3):\n",
    "    classes[i]=classes[i][0:len_classes[i]]\n",
    "for i in range(3):\n",
    "    Y[i]=Y[i][0:len_classes[i]]\n",
    "\n",
    "Train=[]\n",
    "Valid=[]\n",
    "Test=[]\n",
    "TVT=[Train,Valid,Test]\n",
    "Y_Train=[]\n",
    "Y_Valid=[]\n",
    "Y_Test=[]\n",
    "Y_TVT=[Y_Train,Y_Valid,Y_Test]\n",
    "\n",
    "bounds=[0,0.64,0.8,1]\n",
    "for i in range(3):\n",
    "    for j in range(3):\n",
    "        TVT[i]+=classes[j][int(bounds[i]*len_classes[j]):int(bounds[i+1]*len_classes[j])]\n",
    "        Y_TVT[i]+=Y[j][int(bounds[i]*len_classes[j]):int(bounds[i+1]*len_classes[j])]\n",
    "        \n",
    "Augmented=[]\n",
    "Y_Augmented=[]\n",
    "\n",
    "for i in range(len(Train)):\n",
    "    augmentation(Train[i],Y_Train[i],Augmented,Y_Augmented)\n",
    "\n",
    "print(len(Train))\n",
    "\n",
    "TVT[0]=Augmented\n",
    "Y_TVT[0]=Y_Augmented\n",
    "\n",
    "for i in range(3):\n",
    "    TVT[i]=normalize(TVT[i])\n",
    "    TVT[i]=TVT[i].reshape(-1,200,200,1)\n",
    "    Y_TVT[i]=keras.utils.to_categorical(Y_TVT[i],3)\n",
    "\n",
    "print(len(Augmented))\n",
    "\n",
    "# epoch=32 steps_per_epoch=32\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "def train(epochs_num,batch_len,conv_neurons,art_neurons):\n",
    "    \n",
    "    def mlp_model():\n",
    "        Inp=layers.Input(shape=(200,200,1))\n",
    "        hidden=Inp\n",
    "        for conv_neuron in conv_neurons:\n",
    "            hidden=layers.Conv2D(conv_neuron,kernel_size=(3,3),padding='valid',activation='relu')(hidden)\n",
    "            hidden=layers.MaxPool2D(pool_size=(2,2),strides=(2,2))(hidden)\n",
    "        hidden=layers.Flatten()(hidden)\n",
    "        inputs=100*100\n",
    "        w=math.sqrt(inputs)\n",
    "        for art_neuron in art_neurons:\n",
    "            hidden=layers.Dense(art_neuron,activation='relu',kernel_regularizer=regularizers.L2(0.0001))(hidden)\n",
    "            hidden=layers.Dropout(0.2)(hidden)\n",
    "            inputs=30\n",
    "            w=math.sqrt(inputs)\n",
    "        output=layers.Dense(3,activation='softmax',kernel_regularizer=regularizers.L2(0.0001))(hidden)\n",
    "        model=keras.Model(Inp,output)\n",
    "        model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "        return model\n",
    "    \"\"\"\n",
    "    checkpoint = tf.keras.callbacks.ModelCheckpoint(\"weights.hdf5\",monitor='val_accuracy', verbose=1, \n",
    "    save_best_only=True, mode='max')\n",
    "    callbacks_list = [checkpoint]\n",
    "    estimator = KerasClassifier(build_fn=mlp_model, epochs=int(len(X_train)/dividor_epoch), batch_size=batch_len, verbose=2)\n",
    "    kfold = KFold(n_splits=8, shuffle=True, random_state=7)\n",
    "    results = cross_val_score(estimator, X_train, Y_train, cv=kfold)\n",
    "    print(\"MLP: %.2f%% (%.2f%%)\" % (results.mean()*100, results.std()*100))\n",
    "    \"\"\"\n",
    "    callback = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=4,mode='max')\n",
    "    model=mlp_model()\n",
    "    model.fit(TVT[0],Y_TVT[0],validation_data=(TVT[1],Y_TVT[1]),epochs=epochs_num,batch_size=batch_len,callbacks=[callback],verbose=1)\n",
    "    #model.load_weights(\"weights.hdf5\")\n",
    "    model.summary()\n",
    "    score= model.evaluate(TVT[2], Y_TVT[2],verbose=0)\n",
    "    print(\"Test loss:\", score[0])\n",
    "    print(\"Test accuracy:\",score[1])\n",
    "    \n",
    "train(100,150,[32,64,96,128],[50,50])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed4697ad",
   "metadata": {},
   "source": [
    "#### Now we wil test it with 2 dense layers smaller batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48192de5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1493, 2780, 1583]\n",
      "3747\n",
      "14988\n",
      "Epoch 1/100\n",
      "150/150 [==============================] - 760s 5s/step - loss: 1.0496 - accuracy: 0.4856 - val_loss: 0.7540 - val_accuracy: 0.6713\n",
      "Epoch 2/100\n",
      "150/150 [==============================] - 788s 5s/step - loss: 0.9637 - accuracy: 0.5366 - val_loss: 0.5451 - val_accuracy: 0.7545\n",
      "Epoch 3/100\n",
      "150/150 [==============================] - 790s 5s/step - loss: 0.9414 - accuracy: 0.5458 - val_loss: 0.5392 - val_accuracy: 0.7695\n",
      "Epoch 4/100\n",
      "150/150 [==============================] - 846s 6s/step - loss: 0.9350 - accuracy: 0.5477 - val_loss: 0.5075 - val_accuracy: 0.7876\n",
      "Epoch 5/100\n",
      "150/150 [==============================] - 801s 5s/step - loss: 0.9289 - accuracy: 0.5514 - val_loss: 0.4945 - val_accuracy: 0.7855\n",
      "Epoch 6/100\n",
      "150/150 [==============================] - 801s 5s/step - loss: 0.9219 - accuracy: 0.5523 - val_loss: 0.4944 - val_accuracy: 0.7887\n",
      "Epoch 7/100\n",
      "150/150 [==============================] - 798s 5s/step - loss: 0.9170 - accuracy: 0.5548 - val_loss: 0.4688 - val_accuracy: 0.7962\n",
      "Epoch 8/100\n",
      "150/150 [==============================] - 808s 5s/step - loss: 0.9119 - accuracy: 0.5559 - val_loss: 0.5039 - val_accuracy: 0.7962\n",
      "Epoch 9/100\n",
      "150/150 [==============================] - 797s 5s/step - loss: 0.9108 - accuracy: 0.5572 - val_loss: 0.4794 - val_accuracy: 0.8015\n",
      "Epoch 10/100\n",
      "150/150 [==============================] - 831s 6s/step - loss: 0.9042 - accuracy: 0.5588 - val_loss: 0.4817 - val_accuracy: 0.8047\n",
      "Epoch 11/100\n",
      "150/150 [==============================] - 873s 6s/step - loss: 0.9030 - accuracy: 0.5600 - val_loss: 0.5468 - val_accuracy: 0.7844\n",
      "Epoch 12/100\n",
      " 13/150 [=>............................] - ETA: 14:13 - loss: 0.8634 - accuracy: 0.5831"
     ]
    }
   ],
   "source": [
    "#200,200\n",
    "def process(X,file):\n",
    "    img=cv2.imread(file,cv2.IMREAD_GRAYSCALE)\n",
    "    X.append(img)\n",
    "    #write your code\n",
    "\n",
    "def normalize(dataset):\n",
    "    dataset=asarray(dataset).astype('float32')\n",
    "    dataset=dataset/255.0\n",
    "    return dataset\n",
    "\n",
    "def augmentation(row,label,Augmented,Y_Augmeneted):\n",
    "    scale_out = skimage.transform.rescale(row, scale=2.0, mode='constant')\n",
    "    scale_out=skimage.transform.resize(scale_out,(200,200))\n",
    "    scale_in= skimage.transform.rescale(row, scale=0.5, mode='constant')\n",
    "    scale_in=skimage.transform.resize(scale_in,(200,200))\n",
    "    rot = skimage.transform.rotate(row, angle=20, mode='reflect')\n",
    "    rot=skimage.transform.resize(rot,(200,200))\n",
    "    images=[row,scale_out,scale_in,rot]\n",
    "    for image in images:\n",
    "        Augmented.append(image)\n",
    "        Y_Augmented.append(label)\n",
    "\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "import skimage\n",
    "from skimage import transform\n",
    "from skimage.transform import resize\n",
    "from skimage.transform import rescale\n",
    "import cv2\n",
    "import glob\n",
    "import random as ra\n",
    "import PIL\n",
    "from PIL import Image\n",
    "from numpy import asarray\n",
    "\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow import keras\n",
    "from keras import layers\n",
    "from keras import utils\n",
    "import sklearn.metrics as metrics\n",
    "import numpy as np\n",
    "from tensorflow.keras import regularizers\n",
    "import math\n",
    "\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "Normal=[]\n",
    "Bacterial=[]\n",
    "Virus=[]\n",
    "classes=[Virus,Bacterial,Normal]\n",
    "Y_Normal=[]\n",
    "Y_Bacterial=[]\n",
    "Y_Virus=[]\n",
    "Y=[Y_Normal,Y_Bacterial,Y_Virus]\n",
    "files=glob.glob('chest_xray_lower_dim200x200'+'/*.jpeg')\n",
    "ra.shuffle(files)\n",
    "for file in files:\n",
    "    k=-1\n",
    "    if \"virus\" in file:\n",
    "        k=0\n",
    "    elif \"bacteria\" in file:\n",
    "        k=1\n",
    "    else:\n",
    "        k=2\n",
    "    Y[k].append(str(k))\n",
    "    process(classes[k],file)\n",
    "    \n",
    "len_classes=[]\n",
    "for class_ in classes:\n",
    "    len_classes.append(len(class_))\n",
    "print(len_classes)\n",
    "same_length=min(len_classes)\n",
    "evenly=False\n",
    "\n",
    "if evenly:\n",
    "    for i in range(3):\n",
    "        len_classes[i]=same_length\n",
    "\n",
    "for i in range(3):\n",
    "    classes[i]=classes[i][0:len_classes[i]]\n",
    "for i in range(3):\n",
    "    Y[i]=Y[i][0:len_classes[i]]\n",
    "\n",
    "Train=[]\n",
    "Valid=[]\n",
    "Test=[]\n",
    "TVT=[Train,Valid,Test]\n",
    "Y_Train=[]\n",
    "Y_Valid=[]\n",
    "Y_Test=[]\n",
    "Y_TVT=[Y_Train,Y_Valid,Y_Test]\n",
    "\n",
    "bounds=[0,0.64,0.8,1]\n",
    "for i in range(3):\n",
    "    for j in range(3):\n",
    "        keeped=classes[j][int(bounds[i]*len_classes[j]):int(bounds[i+1]*len_classes[j])]\n",
    "        keeped_Y=Y[j][int(bounds[i]*len_classes[j]):int(bounds[i+1]*len_classes[j])]\n",
    "        if(i==0):\n",
    "            keeped=keeped[0:int(len(keeped)/2)]\n",
    "            keeped_Y=keeped_Y[0:int(len(keeped)/2)]                 \n",
    "        TVT[i]+=keeped\n",
    "        Y_TVT[i]+=keeped_Y\n",
    "        \n",
    "Augmented=[]\n",
    "Y_Augmented=[]\n",
    "\n",
    "for i in range(len(Train)):\n",
    "    augmentation(Train[i],Y_Train[i],Augmented,Y_Augmented)\n",
    "\n",
    "print(len(Train))\n",
    "\n",
    "TVT[0]=Augmented\n",
    "Y_TVT[0]=Y_Augmented\n",
    "TVT[0],Y_TVT[0]=shuffle(TVT[0],Y_TVT[0])\n",
    "\n",
    "for i in range(3):\n",
    "    TVT[i]=normalize(TVT[i])\n",
    "    TVT[i]=TVT[i].reshape(-1,200,200,1)\n",
    "    Y_TVT[i]=keras.utils.to_categorical(Y_TVT[i],3)\n",
    "\n",
    "print(len(Augmented))\n",
    "\n",
    "# epoch=32 steps_per_epoch=32\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "def train(epochs_num,batch_len,conv_neurons,art_neurons):\n",
    "    \n",
    "    def mlp_model():\n",
    "        Inp=layers.Input(shape=(200,200,1))\n",
    "        hidden=Inp\n",
    "        for conv_neuron in conv_neurons:\n",
    "            hidden=layers.Conv2D(conv_neuron,kernel_size=(3,3),padding='valid',activation='relu')(hidden)\n",
    "            hidden=BatchNormalization()(hidden)\n",
    "            hidden=layers.MaxPool2D(pool_size=(2,2),strides=(2,2))(hidden)\n",
    "        hidden=layers.Flatten()(hidden)\n",
    "        inputs=100*100\n",
    "        w=math.sqrt(inputs)\n",
    "        for art_neuron in art_neurons:\n",
    "            hidden=layers.Dense(art_neuron,activation='relu',kernel_regularizer=regularizers.L2(0.0001))(hidden)\n",
    "            hidden=layers.Dropout(0.2)(hidden)\n",
    "            inputs=30\n",
    "            w=math.sqrt(inputs)\n",
    "        output=layers.Dense(3,activation='softmax',kernel_regularizer=regularizers.L2(0.0001))(hidden)\n",
    "        model=keras.Model(Inp,output)\n",
    "        model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "        return model\n",
    "    \"\"\"\n",
    "    checkpoint = tf.keras.callbacks.ModelCheckpoint(\"weights.hdf5\",monitor='val_accuracy', verbose=1, \n",
    "    save_best_only=True, mode='max')\n",
    "    callbacks_list = [checkpoint]\n",
    "    estimator = KerasClassifier(build_fn=mlp_model, epochs=int(len(X_train)/dividor_epoch), batch_size=batch_len, verbose=2)\n",
    "    kfold = KFold(n_splits=8, shuffle=True, random_state=7)\n",
    "    results = cross_val_score(estimator, X_train, Y_train, cv=kfold)\n",
    "    print(\"MLP: %.2f%% (%.2f%%)\" % (results.mean()*100, results.std()*100))\n",
    "    \"\"\"\n",
    "    callback = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=4,mode='max')\n",
    "    model=mlp_model()\n",
    "    model.fit(TVT[0],Y_TVT[0],validation_data=(TVT[1],Y_TVT[1]),epochs=epochs_num,batch_size=batch_len,callbacks=[callback],verbose=1)\n",
    "    #model.load_weights(\"weights.hdf5\")\n",
    "    model.summary()\n",
    "    score= model.evaluate(TVT[2], Y_TVT[2],verbose=0)\n",
    "    print(\"Test loss:\", score[0])\n",
    "    print(\"Test accuracy:\",score[1])\n",
    "    \n",
    "train(100,100,[32,64,96,128],[40,30])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f29fdf63",
   "metadata": {},
   "source": [
    "#### Try with ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "97e43522",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "bad operand type for unary -: 'list'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [17]\u001b[0m, in \u001b[0;36m<cell line: 62>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     70\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m3\u001b[39m):\n\u001b[1;32m     71\u001b[0m         Y[k]\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mstr\u001b[39m(k))\n\u001b[0;32m---> 72\u001b[0m     \u001b[43mprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclasses\u001b[49m\u001b[43m[\u001b[49m\u001b[43mk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43mdatagen\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     74\u001b[0m len_classes\u001b[38;5;241m=\u001b[39m[]\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m class_ \u001b[38;5;129;01min\u001b[39;00m classes:\n",
      "Input \u001b[0;32mIn [17]\u001b[0m, in \u001b[0;36mprocess\u001b[0;34m(X, file, gen)\u001b[0m\n\u001b[1;32m     45\u001b[0m it\u001b[38;5;241m=\u001b[39mgen\u001b[38;5;241m.\u001b[39mflow(img_array, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m)\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m2\u001b[39m):\n\u001b[0;32m---> 47\u001b[0m     batch\u001b[38;5;241m=\u001b[39m\u001b[43mit\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnext\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     48\u001b[0m     X\u001b[38;5;241m.\u001b[39mappend(load_img(batch[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muint8\u001b[39m\u001b[38;5;124m\"\u001b[39m)),color_mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgrayscale\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/notebook/jupyterenv/lib/python3.10/site-packages/keras/preprocessing/image.py:160\u001b[0m, in \u001b[0;36mIterator.next\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    157\u001b[0m   index_array \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex_generator)\n\u001b[1;32m    158\u001b[0m \u001b[38;5;66;03m# The transformation of images is not under thread lock\u001b[39;00m\n\u001b[1;32m    159\u001b[0m \u001b[38;5;66;03m# so it can be done in parallel\u001b[39;00m\n\u001b[0;32m--> 160\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_batches_of_transformed_samples\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex_array\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/notebook/jupyterenv/lib/python3.10/site-packages/keras/preprocessing/image.py:708\u001b[0m, in \u001b[0;36mNumpyArrayIterator._get_batches_of_transformed_samples\u001b[0;34m(self, index_array)\u001b[0m\n\u001b[1;32m    706\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(index_array):\n\u001b[1;32m    707\u001b[0m   x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mx[j]\n\u001b[0;32m--> 708\u001b[0m   params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimage_data_generator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_random_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    709\u001b[0m   x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimage_data_generator\u001b[38;5;241m.\u001b[39mapply_transform(\n\u001b[1;32m    710\u001b[0m       x\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdtype), params)\n\u001b[1;32m    711\u001b[0m   x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimage_data_generator\u001b[38;5;241m.\u001b[39mstandardize(x)\n",
      "File \u001b[0;32m~/notebook/jupyterenv/lib/python3.10/site-packages/keras/preprocessing/image.py:1707\u001b[0m, in \u001b[0;36mImageDataGenerator.get_random_transform\u001b[0;34m(self, img_shape, seed)\u001b[0m\n\u001b[1;32m   1704\u001b[0m   np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mseed(seed)\n\u001b[1;32m   1706\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrotation_range:\n\u001b[0;32m-> 1707\u001b[0m   theta \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39muniform(\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrotation_range\u001b[49m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrotation_range)\n\u001b[1;32m   1708\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1709\u001b[0m   theta \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "\u001b[0;31mTypeError\u001b[0m: bad operand type for unary -: 'list'"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import glob\n",
    "import random as ra\n",
    "import PIL\n",
    "from PIL import Image\n",
    "from numpy import asarray\n",
    "\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow import keras\n",
    "from keras import layers\n",
    "from keras import utils\n",
    "import sklearn.metrics as metrics\n",
    "import numpy as np\n",
    "from tensorflow.keras import regularizers\n",
    "import math\n",
    "\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.utils import load_img,img_to_array\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "datagen = ImageDataGenerator(\n",
    "        rotation_range=[10,20],\n",
    "        zoom_range=[-0.2,0.2],\n",
    "        fill_mode=\"reflect\",\n",
    "        horizontal_flip=True\n",
    "    )\n",
    "\n",
    "def process(X,file,gen):\n",
    "    img=load_img(\n",
    "        file,color_mode=\"grayscale\"\n",
    "    )\n",
    "    X.append(img)\n",
    "    \n",
    "    img_array = img_to_array(img)\n",
    "    img_array = img_array.reshape((1,) + img_array.shape)\n",
    "    it=gen.flow(img_array, batch_size=3)\n",
    "    for i in range(2):\n",
    "        batch=it.next()\n",
    "        X.append(load_img(batch[0].astype(\"uint8\")),color_mode=\"grayscale\")\n",
    "    \n",
    "    \n",
    "\n",
    "Normal=[]\n",
    "Bacterial=[]\n",
    "Virus=[]\n",
    "classes=[Virus,Bacterial,Normal]\n",
    "Y_Normal=[]\n",
    "Y_Bacterial=[]\n",
    "Y_Virus=[]\n",
    "Y=[Y_Normal,Y_Bacterial,Y_Virus]\n",
    "files=glob.glob('chest_xray_lower_dim2'+'/*.jpeg')\n",
    "ra.shuffle(files)\n",
    "for file in files:\n",
    "    k=-1\n",
    "    if \"virus\" in file:\n",
    "        k=0\n",
    "    elif \"bacteria\" in file:\n",
    "        k=1\n",
    "    else:\n",
    "        k=2\n",
    "    for i in range(3):\n",
    "        Y[k].append(str(k))\n",
    "    process(classes[k],file,datagen)\n",
    "    \n",
    "len_classes=[]\n",
    "for class_ in classes:\n",
    "    len_classes.append(len(class_))\n",
    "print(len_classes)\n",
    "same_length=min(len_classes)\n",
    "evenly=False\n",
    "\n",
    "if evenly:\n",
    "    for i in range(3):\n",
    "        len_classes[i]=same_length\n",
    "\n",
    "for i in range(3):\n",
    "    classes[i]=classes[i][0:len_classes[i]]\n",
    "for i in range(3):\n",
    "    Y[i]=Y[i][0:len_classes[i]]\n",
    "\n",
    "Train=[]\n",
    "Valid=[]\n",
    "Test=[]\n",
    "TVT=[Train,Valid,Test]\n",
    "Y_Train=[]\n",
    "Y_Valid=[]\n",
    "Y_Test=[]\n",
    "Y_TVT=[Y_Train,Y_Valid,Y_Test]\n",
    "\n",
    "bounds=[0,0.64,0.8,1]\n",
    "for i in range(3):\n",
    "    for j in range(3):\n",
    "        TVT[i]+=classes[j][int(bounds[i]*len_classes[j]):int(bounds[i+1]*len_classes[j])]\n",
    "        Y_TVT[i]+=Y[j][int(bounds[i]*len_classes[j]):int(bounds[i+1]*len_classes[j])]\n",
    "    Y_TVT[i]=keras.utils.to_categorical(Y_TVT[i],3)\n",
    "\n",
    "#train_datagen.fit(Train)\n",
    "#validation_datagen.fit(Valid)\n",
    "#test_datagen.fit(Test)\n",
    "\n",
    "def train(epochs_num,batch_len,conv_neurons,art_neurons):\n",
    "    \n",
    "    def mlp_model():\n",
    "        Inp=layers.Input(shape=(100,100,1))\n",
    "        hidden=Inp\n",
    "        for conv_neuron in conv_neurons:\n",
    "            hidden=layers.Conv2D(conv_neuron,kernel_size=(3,3),padding='valid',activation='relu')(hidden)\n",
    "            hidden=layers.MaxPool2D(pool_size=(2,2),strides=(2,2))(hidden)\n",
    "        hidden=layers.Flatten()(hidden)\n",
    "        inputs=100*100\n",
    "        w=math.sqrt(inputs)\n",
    "        for art_neuron in art_neurons:\n",
    "            i2=i+1\n",
    "            hidden=layers.Dense(art_neuron,activation='relu',kernel_regularizer=regularizers.L2(0.0001))(hidden)\n",
    "            #hidden=layers.Dropout(0.2)(hidden)\n",
    "            inputs=30\n",
    "            w=math.sqrt(inputs)\n",
    "        output=layers.Dense(3,activation='softmax',kernel_regularizer=regularizers.L2(0.0001))(hidden)\n",
    "        model=keras.Model(Inp,output)\n",
    "        model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "        return model\n",
    "    \"\"\"\n",
    "    checkpoint = tf.keras.callbacks.ModelCheckpoint(\"weights.hdf5\",monitor='val_accuracy', verbose=1, \n",
    "    save_best_only=True, mode='max')\n",
    "    callbacks_list = [checkpoint]\n",
    "    estimator = KerasClassifier(build_fn=mlp_model, epochs=int(len(X_train)/dividor_epoch), batch_size=batch_len, verbose=2)\n",
    "    kfold = KFold(n_splits=8, shuffle=True, random_state=7)\n",
    "    results = cross_val_score(estimator, X_train, Y_train, cv=kfold)\n",
    "    print(\"MLP: %.2f%% (%.2f%%)\" % (results.mean()*100, results.std()*100))\n",
    "    \"\"\"\n",
    "    callback = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=4,mode='max')\n",
    "    model=mlp_model()\n",
    "    model.fit(TVT[0],Y_TVT[0],validation_data=(validation_datagen.flow(TVT[1]),Y_TVT[1]),epochs=epochs_num,\n",
    "              batch_size=batch_len,callbacks=[callback],verbose=1)\n",
    "    #model.load_weights(\"weights.hdf5\")\n",
    "    model.summary()\n",
    "    score= model.evaluate(test_datagen.flow(TVT[2]), Y_TVT[2],verbose=0)\n",
    "    print(\"Test loss:\", score[0])\n",
    "    print(\"Test accuracy:\",score[1])\n",
    "    \n",
    "train(100,150,[32,64,96],[50,50])\n",
    "\n",
    "\n",
    "\n",
    "   \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41e0ee04",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
