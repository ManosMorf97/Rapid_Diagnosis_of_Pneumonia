{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d487406d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(directory):\n",
    "    #before work\n",
    "    import cv2\n",
    "    import glob\n",
    "    files=glob.glob(directory+'/*.jpeg')\n",
    "    dataset=[]\n",
    "    labels=[]\n",
    "    for file in files:\n",
    "        img=cv2.imread(file,cv2.IMREAD_GRAYSCALE)\n",
    "        dataset.append(img)\n",
    "        #write your code\n",
    "        if \"virus\" in file:\n",
    "            labels.append(\"0\")\n",
    "        elif \"bacteria\" in file:\n",
    "            labels.append(\"1\")\n",
    "        else:\n",
    "            labels.append(\"2\")\n",
    "\n",
    "    from numpy import asarray\n",
    "    dataset=asarray(dataset).astype('float32')\n",
    "    dataset=dataset/255.0\n",
    "    return dataset,labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d0215e38",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset,labels=create_dataset('chest_xray_lower_dim')\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow import keras\n",
    "from keras import layers\n",
    "from keras import utils\n",
    "import sklearn.metrics as metrics\n",
    "import numpy as np\n",
    "from tensorflow.keras import regularizers\n",
    "import math\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1816eb9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "\n",
    "def train(epochs_num,batch_len):\n",
    "    X_train,X_test,Y_train,Y_test=train_test_split(\n",
    "    dataset, labels, test_size=0.2, random_state=42)\n",
    "    X_train=X_train.reshape(-1,100,100,1)\n",
    "    X_test=X_test.reshape(-1,100,100,1)\n",
    "    Y_train = keras.utils.to_categorical(Y_train,3)\n",
    "    Y_test = keras.utils.to_categorical(Y_test,3)\n",
    "    \n",
    "    \n",
    "    def mlp_model():\n",
    "        Inp=layers.Input(shape=(400,400,1))\n",
    "        hidden=Inp\n",
    "        for i in range(2):\n",
    "            hidden=layers.Conv2D(10,kernel_size=(3,3),padding='valid',activation='relu')(hidden)\n",
    "            hidden=layers.MaxPool2D(pool_size=(2,2),strides=(2,2))(hidden)\n",
    "        hidden=layers.Conv2D(10,kernel_size=(3,3),padding='valid',activation='relu')(hidden)\n",
    "        hidden=layers.MaxPool2D(pool_size=(2,2),strides=(1,1))(hidden)\n",
    "        hidden=layers.Flatten()(hidden)\n",
    "        for i in range(5):\n",
    "            hidden=layers.Dense(30,activation=\"relu\",kernel_initializer=tf.keras.initializers.RandomUniform(minval=-1/math.pow(76.4,math.pow(i+1,-1)), maxval=1/math.pow(76.4,math.pow(i+1,-1)), seed=None))(hidden)\n",
    "            #hidden=layers.Dropout(0.2)(hidden)\n",
    "        output=layers.Dense(3,activation=\"softmax\",kernel_initializer=tf.keras.initializers.RandomUniform(minval=-1/math.pow(76.4,math.pow(4,-1)), maxval=1/math.pow(76.4,math.pow(4,-1)), seed=None))(hidden)\n",
    "        model=keras.Model(Inp,output)\n",
    "        model.compile(loss=\"categorical_crossentropy\",optimizer='adam',metrics=[\"accuracy\"])\n",
    "        return model\n",
    "    \"\"\"\n",
    "    checkpoint = tf.keras.callbacks.ModelCheckpoint(\"weights.hdf5\",monitor='val_accuracy', verbose=1, \n",
    "    save_best_only=True, mode='max')\n",
    "    callbacks_list = [checkpoint]\n",
    "    estimator = KerasClassifier(build_fn=mlp_model, epochs=int(len(X_train)/dividor_epoch), batch_size=batch_len, verbose=2)\n",
    "    kfold = KFold(n_splits=8, shuffle=True, random_state=7)\n",
    "    results = cross_val_score(estimator, X_train, Y_train, cv=kfold)\n",
    "    print(\"MLP: %.2f%% (%.2f%%)\" % (results.mean()*100, results.std()*100))\n",
    "    \"\"\"\n",
    "    model=mlp_model()\n",
    "    model.fit(X_train,Y_train,validation_split=0.2,epochs=epochs_num,batch_size=batch_len,verbose=2)\n",
    "    #model.load_weights(\"weights.hdf5\")\n",
    "    model.summary()\n",
    "    score= model.evaluate(X_test, Y_test,verbose=0)\n",
    "    print(\"Test loss:\", score[0])\n",
    "    print(\"Test accuracy:\",score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "140ff1f8",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Data cardinality is ambiguous:\n  x sizes: 59955\n  y sizes: 4684\nMake sure all arrays contain the same number of samples.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_20568/2441904813.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m60\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m200\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_20568/864712451.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(epochs_num, batch_len)\u001b[0m\n\u001b[0;32m     40\u001b[0m     \"\"\"\n\u001b[0;32m     41\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmlp_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 42\u001b[1;33m     \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mY_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mvalidation_split\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mepochs_num\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch_len\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     43\u001b[0m     \u001b[1;31m#model.load_weights(\"weights.hdf5\")\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     44\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\keras\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     65\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 67\u001b[1;33m       \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     68\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m       \u001b[1;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\keras\\engine\\data_adapter.py\u001b[0m in \u001b[0;36m_check_data_cardinality\u001b[1;34m(data)\u001b[0m\n\u001b[0;32m   1651\u001b[0m                            for i in tf.nest.flatten(single_data)))\n\u001b[0;32m   1652\u001b[0m     \u001b[0mmsg\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;34m\"Make sure all arrays contain the same number of samples.\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1653\u001b[1;33m     \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1654\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1655\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Data cardinality is ambiguous:\n  x sizes: 59955\n  y sizes: 4684\nMake sure all arrays contain the same number of samples."
     ]
    }
   ],
   "source": [
    "train(60,200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3030545",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
